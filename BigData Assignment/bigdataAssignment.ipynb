{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explain the core components of the Hadoop ecosystem and their respective roles in processing and storing big data. Provide a brief overview of HDFS, MapReduce, and YARN.\n",
    "\n",
    "\n",
    "The Hadoop ecosystem is an open-source framework designed for processing and storing big data efficiently. It comprises several core components that work together to manage and analyze large datasets distributed across clusters of computers. Below is an overview of the primary components:\n",
    "\n",
    "1. HDFS (Hadoop Distributed File System)\n",
    "Role: Storage\n",
    "HDFS is the storage layer of Hadoop that enables distributed storage of data across multiple machines. It is designed to handle large files and provide high throughput for data access.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Distributed Architecture: Data is split into blocks (default size: 128 MB or 256 MB) and stored across multiple nodes.\n",
    "Fault Tolerance: Replicates data blocks (default replication factor: 3) to ensure data availability even if a node fails.\n",
    "Scalability: Can scale horizontally by adding more nodes to the cluster.\n",
    "Write-Once, Read-Many Model: Optimized for large-scale sequential reads rather than frequent updates.\n",
    "2. MapReduce\n",
    "Role: Processing\n",
    "MapReduce is the data processing engine in Hadoop. It provides a programming model for processing and generating large datasets in parallel.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Map Phase: Breaks down the input data into smaller chunks and processes them in parallel. The output of this phase is a set of intermediate key-value pairs.\n",
    "Reduce Phase: Aggregates and processes the intermediate key-value pairs to produce the final output.\n",
    "Parallelism: Executes tasks in parallel across nodes for scalability and efficiency.\n",
    "Fault Tolerance: Automatically retries failed tasks on other nodes.\n",
    "3. YARN (Yet Another Resource Negotiator)\n",
    "Role: Resource Management\n",
    "YARN is the resource management layer of Hadoop, responsible for managing and allocating resources (CPU, memory) to various applications running on the cluster.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Resource Allocation: Dynamically allocates resources to applications based on their needs.\n",
    "Job Scheduling: Schedules jobs to maximize cluster resource utilization and performance.\n",
    "Multi-Tenancy: Allows multiple frameworks (e.g., MapReduce, Spark) to run simultaneously on the same cluster.\n",
    "Scalability: Handles large clusters with thousands of nodes efficiently.\n",
    "Integration of HDFS, MapReduce, and YARN\n",
    "HDFS stores the data to be processed, distributing it across multiple nodes.\n",
    "MapReduce performs computation on the data stored in HDFS, leveraging parallel processing.\n",
    "YARN ensures optimal resource utilization and task scheduling, supporting various data processing frameworks.\n",
    "Together, these components form the backbone of the Hadoop ecosystem, enabling scalable and fault-tolerant big data storage and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Discuss the Hadoop Distributed File System (HDFS) in detail. Explain how it stores and manages data in a distributed environment. Describe the key concepts of HDFS, such as NameNode, DataNode, and blocks, and how they contribute to data reliability and fault tolerance.\n",
    "\n",
    "\n",
    "Hadoop Distributed File System (HDFS)\n",
    "HDFS is a distributed file system designed to store and manage large datasets across a cluster of machines while providing high availability, fault tolerance, and scalability. It operates under a master-slave architecture and is optimized for high-throughput access to data, making it well-suited for big data processing.\n",
    "\n",
    "Key Concepts of HDFS\n",
    "1. Blocks\n",
    "Definition: HDFS stores data as blocks, which are fixed-size chunks (default size: 128 MB, configurable). Large files are split into these blocks for distributed storage.\n",
    "Why Blocks?\n",
    "Efficient Storage: Enables large files to be distributed across multiple nodes.\n",
    "Fault Tolerance: Replication of blocks ensures data availability even in case of node failure.\n",
    "Block Replication:\n",
    "Each block is replicated across multiple nodes (default replication factor: 3) to ensure fault tolerance and reliability.\n",
    "If one node storing a block fails, other replicas can be used to retrieve the data.\n",
    "2. NameNode\n",
    "Role:\n",
    "The master node that manages the metadata of the file system (e.g., file names, file permissions, block locations). It does not store the actual data but keeps track of where data blocks are stored across the cluster.\n",
    "Responsibilities:\n",
    "Maintains the namespace of the file system.\n",
    "Maps files to the blocks they consist of and tracks their locations on DataNodes.\n",
    "Ensures data reliability through periodic health checks and re-replication of missing blocks.\n",
    "Single Point of Failure:\n",
    "If the NameNode fails, the entire HDFS becomes inaccessible. However, this is mitigated using Secondary NameNode or High-Availability setups.\n",
    "3. DataNode\n",
    "Role:\n",
    "The worker nodes that store the actual data blocks. Each DataNode communicates with the NameNode and other DataNodes to ensure data integrity.\n",
    "Responsibilities:\n",
    "Handles read and write requests from clients.\n",
    "Periodically sends block reports and heartbeats to the NameNode to confirm its health and block status.\n",
    "Fault Tolerance:\n",
    "If a DataNode fails, the NameNode identifies the missing blocks and initiates replication from available replicas to maintain the replication factor.\n",
    "How HDFS Manages Data in a Distributed Environment\n",
    "Data Storage:\n",
    "\n",
    "When a client uploads a file to HDFS, it is split into blocks, which are then distributed across multiple DataNodes.\n",
    "The NameNode records the block-to-node mapping in its metadata.\n",
    "Fault Tolerance:\n",
    "\n",
    "Data is replicated across multiple nodes.\n",
    "If a DataNode fails, the replicas on other nodes ensure data availability. The NameNode re-replicates blocks if needed.\n",
    "High Throughput:\n",
    "\n",
    "HDFS is optimized for sequential reads and large file processing, enabling high-throughput data access.\n",
    "Write-Once, Read-Many Model:\n",
    "\n",
    "Data written to HDFS is immutable, simplifying consistency and reliability.\n",
    "Key Features for Data Reliability and Fault Tolerance\n",
    "Block Replication:\n",
    "Ensures data is available even if nodes fail. The default replication factor of 3 ensures high reliability.\n",
    "Heartbeats and Block Reports:\n",
    "DataNodes periodically send heartbeats to the NameNode to signal they are functioning. Block reports ensure the NameNode has up-to-date information about blocks.\n",
    "Re-Replication:\n",
    "When a block replica is lost, the NameNode automatically triggers the creation of new replicas on healthy nodes.\n",
    "Benefits of HDFS\n",
    "Scalability: Easily scales horizontally by adding more nodes.\n",
    "Fault Tolerance: Ensures data reliability through block replication.\n",
    "Cost-Effective: Designed to run on commodity hardware.\n",
    "High Throughput: Optimized for batch processing and large datasets.\n",
    "HDFS is the foundation of the Hadoop ecosystem, providing reliable, scalable, and distributed storage for big data applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write a step-by-step explanation of how the MapReduce framework works. Use a real-world example to illustrate the Map and Reduce phases. Discuss the advantages and limitations of MapReduce for processing large datasets.\n",
    "\n",
    "MapReduce is a programming model introduced by Google for processing large datasets in a distributed and parallel manner. It divides the processing into two key phases: Map and Reduce, with an intermediate step called Shuffle and Sort. Each phase is performed on a cluster of machines, enabling horizontal scalability and fault tolerance.\n",
    "\n",
    "Core Components of the MapReduce Framework\n",
    "Map Phase:\n",
    "\n",
    "The input data is split into independent chunks, and a mapper function is applied to each chunk.\n",
    "The mapper processes the input and generates key-value pairs as intermediate output.\n",
    "For example: From raw input data like sentences, the mapper can generate word count pairs such as (\"word\", 1).\n",
    "Shuffle and Sort Phase:\n",
    "\n",
    "The intermediate key-value pairs generated by the mappers are grouped by key.\n",
    "The framework ensures that all values associated with the same key are aggregated together and sent to the reducer.\n",
    "Reduce Phase:\n",
    "\n",
    "The reducer takes the grouped key-value pairs and processes them to produce the final output.\n",
    "The reducer aggregates or summarizes data based on the logic defined (e.g., summing up counts for word occurrences).\n",
    "Output:\n",
    "\n",
    "The final output is stored in a distributed file system, typically HDFS, as structured or semi-structured data.\n",
    "Step-by-Step Process\n",
    "Input Splitting:\n",
    "\n",
    "The input data is divided into smaller chunks (splits), with each chunk processed independently.\n",
    "This ensures parallelism and efficient utilization of the cluster.\n",
    "Mapping:\n",
    "\n",
    "Each split is processed by the mapper function, which transforms the input data into intermediate key-value pairs.\n",
    "Shuffling and Sorting:\n",
    "\n",
    "The framework automatically groups key-value pairs by key and sorts them to ensure all values for a given key are collated.\n",
    "Reducing:\n",
    "\n",
    "The reducer function aggregates or processes the grouped data, producing the final output.\n",
    "Output Storage:\n",
    "\n",
    "The final results are written to distributed storage for subsequent use.\n",
    "Advantages of MapReduce\n",
    "Simplicity:\n",
    "\n",
    "Abstracts the complexity of distributed computing into simple Map and Reduce functions.\n",
    "Scalability:\n",
    "\n",
    "Handles massive datasets by distributing the workload across many machines.\n",
    "Fault Tolerance:\n",
    "\n",
    "Automatically manages hardware failures by reassigning tasks to other machines.\n",
    "Cost-Effective:\n",
    "\n",
    "Designed to run on commodity hardware, reducing infrastructure costs.\n",
    "Data Locality:\n",
    "\n",
    "Processes data on the nodes where it resides, minimizing data transfer overhead.\n",
    "Limitations of MapReduce\n",
    "High Latency:\n",
    "\n",
    "Batch-oriented processing results in high latency, making it unsuitable for real-time analytics.\n",
    "I/O Overhead:\n",
    "\n",
    "Intermediate data is written to and read from disk, leading to significant I/O overhead.\n",
    "Iterative Processing:\n",
    "\n",
    "Not ideal for iterative tasks like machine learning, as each iteration requires a new MapReduce job.\n",
    "Complexity for Beginners:\n",
    "\n",
    "Requires knowledge of programming and distributed computing concepts to write Map and Reduce functions.\n",
    "Limited Flexibility:\n",
    "\n",
    "Dependency on HDFS and its fixed workflow limits customization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore the role of YARN in Hadoop. Explain how it manages cluster resources and schedules applications. Compare YARN with the earlier Hadoop 1.x architecture and highlight the benefits of YARN.\n",
    "\n",
    "YARN (Yet Another Resource Negotiator) is the resource management and job scheduling layer of the Hadoop ecosystem, introduced in Hadoop 2.x. It overcomes the limitations of Hadoop 1.x, where MapReduce had a monolithic design, integrating both resource management and processing logic. YARN separates these responsibilities, enabling more efficient resource utilization and supporting multiple processing models.\n",
    "\n",
    "How YARN Manages Cluster Resources\n",
    "Resource Manager (RM):\n",
    "\n",
    "The central authority in YARN that manages resources across the cluster.\n",
    "Divided into:\n",
    "Scheduler: Allocates resources based on capacity, fairness, or other scheduling policies without monitoring task execution.\n",
    "Application Manager: Manages the lifecycle of applications and coordinates with NodeManagers for launching containers.\n",
    "Node Manager (NM):\n",
    "\n",
    "A per-node agent responsible for managing resources on individual nodes.\n",
    "Monitors resource usage (CPU, memory) of containers running on the node and reports to the Resource Manager.\n",
    "Application Master (AM):\n",
    "\n",
    "A per-application process that negotiates resources with the Resource Manager and monitors the execution of tasks.\n",
    "Each application (e.g., a MapReduce job) has its own AM.\n",
    "Containers:\n",
    "\n",
    "The basic unit of resource allocation in YARN.\n",
    "Containers are assigned specific amounts of CPU and memory, within which applications run.\n",
    "How YARN Schedules Applications\n",
    "Resource Allocation:\n",
    "\n",
    "YARN dynamically allocates resources based on application needs.\n",
    "The Resource Manager decides which resources to allocate and assigns containers to applications.\n",
    "Task Scheduling:\n",
    "\n",
    "YARN supports multiple scheduling policies (e.g., FIFO, Capacity Scheduler, Fair Scheduler) to balance resource usage and ensure efficient execution.\n",
    "Fault Tolerance:\n",
    "\n",
    "If a task fails, the Application Master reassigns resources and restarts the task.\n",
    "Comparison of YARN and Hadoop 1.x Architecture\n",
    "Aspect\tHadoop 1.x (Pre-YARN)\tHadoop 2.x (YARN)\n",
    "Resource Management\tIntegrated with MapReduce.\tDecoupled from processing logic.\n",
    "Scalability\tLimited scalability due to single JobTracker.\tHighly scalable with distributed Resource Manager.\n",
    "Fault Tolerance\tJobTracker failure affects all jobs.\tApplication-specific Application Master ensures resilience.\n",
    "Processing Models\tSupports only MapReduce.\tSupports multiple models (e.g., MapReduce, Spark, Tez).\n",
    "Cluster Utilization\tPoor resource utilization due to fixed slots for Map and Reduce tasks.\tDynamic resource allocation for better utilization.\n",
    "Flexibility\tLimited to batch processing.\tSupports real-time, batch, and interactive processing.\n",
    "Benefits of YARN\n",
    "Improved Scalability:\n",
    "\n",
    "YARN eliminates the bottleneck of a single JobTracker, allowing for more nodes and applications in the cluster.\n",
    "Resource Efficiency:\n",
    "\n",
    "Dynamically allocates resources, ensuring optimal usage of cluster capacity.\n",
    "Support for Multiple Workloads:\n",
    "\n",
    "Enables the execution of different types of applications (e.g., Spark, Hive, Tez) alongside MapReduce.\n",
    "Enhanced Fault Tolerance:\n",
    "\n",
    "Application Master handles task failures at the application level, reducing the impact on the cluster.\n",
    "Decoupled Architecture:\n",
    "\n",
    "Separation of resource management and processing logic improves flexibility and maintainability.\n",
    "Multitenancy:\n",
    "\n",
    "Different schedulers (e.g., Fair, Capacity) allow multiple users and teams to share cluster resources effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Provide an overview of some popular components within the Hadoop ecosystem, such as HBase, Hive, Pig, and Spark. Describe the use cases and differences between these components. Choose one component and explain how it can be integrated into a Hadoop ecosystem for specific data processing tasks.\n",
    "\n",
    "The Hadoop ecosystem is enriched with various components designed to handle diverse big data requirements. Some of the popular components include HBase, Hive, Pig, and Spark. Each component serves a unique purpose, offering flexibility in managing and processing large-scale data.\n",
    "\n",
    "Key Components\n",
    "HBase:\n",
    "\n",
    "Type: NoSQL database.\n",
    "Purpose: Stores large amounts of sparse, structured, or semi-structured data in a column-oriented format.\n",
    "Use Cases:\n",
    "Real-time read/write access to big data.\n",
    "Use cases requiring random access to large datasets (e.g., time-series data, IoT).\n",
    "Log or event data storage.\n",
    "Key Features: Built on HDFS, provides fault tolerance, and supports real-time querying.\n",
    "Hive:\n",
    "\n",
    "Type: Data warehousing and SQL-like querying tool.\n",
    "Purpose: Enables querying and managing large datasets stored in HDFS using a SQL-like language (HiveQL).\n",
    "Use Cases:\n",
    "Batch processing and ETL (Extract, Transform, Load) tasks.\n",
    "Data summarization and report generation.\n",
    "Key Features: Easy to learn for SQL users, integrates well with BI tools, supports structured and semi-structured data.\n",
    "Pig:\n",
    "\n",
    "Type: High-level scripting platform.\n",
    "Purpose: Processes large datasets using a scripting language called Pig Latin.\n",
    "Use Cases:\n",
    "Data preprocessing and transformation.\n",
    "Scenarios requiring user-defined functions for custom data operations.\n",
    "Key Features: Simplifies complex MapReduce jobs, suitable for both structured and unstructured data.\n",
    "Spark:\n",
    "\n",
    "Type: Unified analytics engine for big data processing.\n",
    "Purpose: Processes data in-memory, enabling real-time and batch processing.\n",
    "Use Cases:\n",
    "Real-time stream processing (e.g., Twitter sentiment analysis).\n",
    "Machine learning, graph processing, and iterative computations.\n",
    "Key Features: Faster than MapReduce, supports multiple languages (Python, Scala, Java), and integrates with other components like HDFS, Hive, and HBase.\n",
    "Differences Between the Components\n",
    "Aspect\tHBase\tHive\tPig\tSpark\n",
    "Type\tNoSQL database\tData warehousing tool\tScripting platform\tAnalytics engine\n",
    "Data Format\tColumn-oriented storage\tStructured/semi-structured data\tStructured/unstructured\tAll types (structured, semi-structured, unstructured)\n",
    "Processing Type\tReal-time access\tBatch processing\tBatch processing\tBatch and real-time\n",
    "Ease of Use\tAPI-based\tSQL-like\tScript-based\tAPI or libraries\n",
    "Key Feature\tRandom data access\tSQL querying over HDFS\tSimplifies MapReduce\tIn-memory processing\n",
    "Integration of Hive in the Hadoop Ecosystem\n",
    "Use Case: Suppose a retail company wants to analyze customer transactions stored in HDFS to generate sales reports and insights.\n",
    "\n",
    "Integration Steps:\n",
    "\n",
    "Data Loading: Load raw transaction data stored in HDFS into Hive tables.\n",
    "Schema Definition: Use HiveQL to define table schemas for structured storage.\n",
    "Querying: Perform complex SQL-like queries to extract insights (e.g., top-selling products, sales by region).\n",
    "Output: Save query results back into HDFS or integrate with BI tools like Tableau or Power BI.\n",
    "Advantages:\n",
    "\n",
    "Hive abstracts the complexity of writing MapReduce jobs by providing a SQL-like interface.\n",
    "Its integration with HDFS ensures efficient querying over massive datasets.\n",
    "\n",
    "The Hadoop ecosystem comprises versatile tools tailored for specific tasks, from real-time data access (HBase) to high-speed analytics (Spark). Hive is an excellent choice for SQL users who need to perform batch processing on HDFS data, while Spark excels in scenarios requiring real-time processing. The integration of these tools provides a robust framework for big data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Explain the key differences between Apache Spark and Hadoop MapReduce. How does Spark overcome some of the limitations of MapReduce for big data processing tasks?\n",
    "\n",
    "Apache Spark and Hadoop MapReduce are two prominent frameworks for big data processing. While both are designed for handling large-scale data, they differ significantly in their architecture, processing capabilities, and performance.\n",
    "\n",
    "1. Processing Model\n",
    "MapReduce:\n",
    "\n",
    "Follows a disk-based batch processing model.\n",
    "Data is read from and written to HDFS at every stage of computation, resulting in high I/O overhead.\n",
    "Suitable for simple, non-iterative tasks like ETL and log analysis.\n",
    "Spark:\n",
    "\n",
    "Follows an in-memory processing model. Data is loaded into memory and processed iteratively, reducing disk I/O.\n",
    "Supports real-time and iterative computations, making it ideal for machine learning, graph processing, and stream analytics.\n",
    "2. Speed and Performance\n",
    "MapReduce:\n",
    "\n",
    "Slower due to frequent read/write operations to HDFS.\n",
    "Limited performance for iterative tasks as each iteration requires a new MapReduce job.\n",
    "Spark:\n",
    "\n",
    "Up to 100x faster than MapReduce for in-memory computations and 10x faster for disk-based operations.\n",
    "Efficient for iterative algorithms due to its Resilient Distributed Dataset (RDD) architecture.\n",
    "3. Ease of Use\n",
    "MapReduce:\n",
    "\n",
    "Requires writing low-level Java or Python code for each task.\n",
    "Complex programming model makes it harder for developers to implement tasks.\n",
    "Spark:\n",
    "\n",
    "Provides high-level APIs in Python, Scala, Java, and R.\n",
    "Offers built-in libraries for machine learning (MLlib), graph processing (GraphX), and stream processing (Spark Streaming), simplifying development.\n",
    "4. Fault Tolerance\n",
    "MapReduce:\n",
    "\n",
    "Relies on HDFS for fault tolerance. If a task fails, it is restarted from the last checkpoint stored in HDFS.\n",
    "Spark:\n",
    "\n",
    "Uses RDDs to achieve fault tolerance. Lost data can be recomputed from the lineage of operations, ensuring minimal data loss without relying heavily on HDFS.\n",
    "5. Supported Workloads\n",
    "MapReduce:\n",
    "\n",
    "Primarily designed for batch processing tasks.\n",
    "Spark:\n",
    "\n",
    "Supports diverse workloads, including batch processing, real-time streaming, iterative machine learning, and graph computations.\n",
    "6. Resource Utilization\n",
    "MapReduce:\n",
    "\n",
    "Limited resource utilization as tasks are executed in sequential stages with fixed resource allocation.\n",
    "Spark:\n",
    "\n",
    "Dynamically allocates resources and executes tasks in parallel, ensuring efficient utilization of the cluster.\n",
    "How Spark Overcomes Limitations of MapReduce\n",
    "Reduced I/O Overhead:\n",
    "\n",
    "Spark's in-memory computation significantly reduces the I/O bottleneck present in MapReduce, where data is read/written to HDFS between stages.\n",
    "Real-Time Processing:\n",
    "\n",
    "Spark Streaming allows real-time data processing, which is not possible with MapReduce.\n",
    "Iterative Processing:\n",
    "\n",
    "MapReduce requires a new job for each iteration, while Spark processes iterations within the same application using in-memory RDDs.\n",
    "Unified Framework:\n",
    "\n",
    "Spark integrates batch processing, stream processing, and advanced analytics into a single framework, whereas MapReduce is limited to batch processing.\n",
    "Ease of Development:\n",
    "\n",
    "High-level APIs and built-in libraries in Spark simplify the development process, allowing developers to focus on logic rather than low-level implementation.\n",
    "\n",
    "While Hadoop MapReduce laid the foundation for big data processing, Apache Spark addresses its limitations with faster performance, ease of use, and support for diverse workloads. Spark's ability to process data in memory and handle real-time streams makes it the preferred choice for modern big data applications, especially in scenarios requiring speed, scalability, and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write a Spark application in Scala or Python that reads a text file, counts the occurrences of each word, and returns the top 10 most frequent words. Explain the key components and steps involved in this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Word Count Application\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Read the text file into an RDD\n",
    "file_path = \"path/to/your/textfile.txt\"\n",
    "text_rdd = spark.sparkContext.textFile(file_path)\n",
    "\n",
    "# Step 3: Transform the data\n",
    "word_counts = (\n",
    "    text_rdd.flatMap(lambda line: line.split())  # Split each line into words\n",
    "    .map(lambda word: (word.lower(), 1))        # Convert words to lowercase and map them to (word, 1)\n",
    "    .reduceByKey(lambda a, b: a + b)            # Aggregate word counts\n",
    ")\n",
    "\n",
    "# Step 4: Get the top 10 most frequent words\n",
    "top_10_words = word_counts \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .take(10)\n",
    "\n",
    "# Step 5: Print the results\n",
    "for word, count in top_10_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Step 6: Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Components and Steps Involved\n",
    "1. SparkSession\n",
    "Acts as the entry point to use Spark. It initializes the Spark environment and provides access to the SparkContext for working with RDDs.\n",
    "In the code:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "spark = SparkSession.builder.appName(\"Word Count Application\").getOrCreate()\n",
    "2. Reading Data\n",
    "The text file is loaded into an RDD (Resilient Distributed Dataset), which is Spark's core abstraction for distributed data.\n",
    "In the code:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "text_rdd = spark.sparkContext.textFile(file_path)\n",
    "3. Transformation Steps\n",
    "flatMap: Splits each line into individual words. Unlike map, it flattens the results into a single list.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "text_rdd.flatMap(lambda line: line.split())\n",
    "map: Maps each word to a tuple (word, 1) for counting.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    ".map(lambda word: (word.lower(), 1))\n",
    "reduceByKey: Aggregates counts for each word by summing up the values.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    ".reduceByKey(lambda a, b: a + b)\n",
    "4. Action Steps\n",
    "sortBy: Sorts the words by their counts in descending order.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    ".sortBy(lambda x: x[1], ascending=False)\n",
    "take: Collects the top 10 results from the sorted RDD.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    ".take(10)\n",
    "5. Output\n",
    "The results are printed in the form word: count.\n",
    "6. Closing Resources\n",
    "Stopping the SparkSession ensures the application releases cluster resources.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Using Spark RDDs (Resilient Distributed Datasets), perform the following tasks on a dataset of your choice: a. Filter the data to select only rows that meet specific criteria. b. Map a transformation to modify a specific column in the dataset. c. Reduce the dataset to calculate a meaningful aggregation (e.g., sum, average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Step 1: Initialize a SparkSession\u001b[39;00m\n\u001b[0;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD Operations Example\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD Operations Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Create an RDD (simulating loading a dataset)\n",
    "data = [\n",
    "    (\"T1\", \"Laptop\", \"Electronics\", 1000, 2),\n",
    "    (\"T2\", \"Phone\", \"Electronics\", 800, 1),\n",
    "    (\"T3\", \"Shirt\", \"Clothing\", 50, 5),\n",
    "    (\"T4\", \"Shoes\", \"Clothing\", 120, 3),\n",
    "    (\"T5\", \"Headphones\", \"Electronics\", 150, 4)\n",
    "]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Step 3a: Filter rows where Category is \"Electronics\"\n",
    "filtered_rdd = rdd.filter(lambda row: row[2] == \"Electronics\")\n",
    "\n",
    "# Step 3b: Map transformation to calculate Total Price (Price * Quantity) for each row\n",
    "mapped_rdd = filtered_rdd.map(lambda row: (row[0], row[1], row[2], row[3], row[4], row[3] * row[4]))\n",
    "\n",
    "# Step 3c: Reduce to calculate the total revenue for \"Electronics\"\n",
    "total_revenue = mapped_rdd.map(lambda row: row[5]).reduce(lambda a, b: a + b)\n",
    "\n",
    "# Step 4: Print results\n",
    "print(\"Filtered Rows (Category: Electronics):\")\n",
    "for row in filtered_rdd.collect():\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nMapped Rows with Total Price:\")\n",
    "for row in mapped_rdd.collect():\n",
    "    print(row)\n",
    "\n",
    "print(f\"\\nTotal Revenue for Electronics: {total_revenue}\")\n",
    "\n",
    "# Step 5: Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Create a Spark DataFrame in Python or Scala by loading a dataset (e.g., CSV or JSON) and perform the following operations: a. Select specific columns from the DataFrame.  b. Filter rows based on certain conditions  c. Group the data by a particular column and calculate aggregations (e.g., sum, average).  d. Join two DataFrames based on a common key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, avg, \u001b[38;5;28msum\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: Initialize SparkSession\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark DataFrame Operations Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load CSV datasets into Spark DataFrames\n",
    "sales_df = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n",
    "customers_df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 3a: Select specific columns (e.g., Product and Price)\n",
    "selected_columns_df = sales_df.select(\"Product\", \"Price\")\n",
    "\n",
    "# Step 3b: Filter rows where Category is \"Electronics\" and Price > 500\n",
    "filtered_df = sales_df.filter((col(\"Category\") == \"Electronics\") & (col(\"Price\") > 500))\n",
    "\n",
    "# Step 3c: Group data by Category and calculate total and average Price\n",
    "aggregated_df = sales_df.groupBy(\"Category\").agg(\n",
    "    sum(\"Price\").alias(\"Total_Price\"),\n",
    "    avg(\"Price\").alias(\"Average_Price\")\n",
    ")\n",
    "\n",
    "# Step 3d: Join sales_df and customers_df on TransactionID\n",
    "joined_df = sales_df.join(customers_df, on=\"TransactionID\", how=\"inner\")\n",
    "\n",
    "# Step 4: Display results\n",
    "print(\"Selected Columns:\")\n",
    "selected_columns_df.show()\n",
    "\n",
    "print(\"Filtered Rows (Category: Electronics and Price > 500):\")\n",
    "filtered_df.show()\n",
    "\n",
    "print(\"Aggregated Data (Group by Category):\")\n",
    "aggregated_df.show()\n",
    "\n",
    "print(\"Joined DataFrames:\")\n",
    "joined_df.show()\n",
    "\n",
    "# Step 5: Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Set up a Spark Streaming application to process real-time data from a source (e.g., Apache Kafka or a simulated data source). The application should: a. Ingest data in micro-batches. b. Apply a transformation to the streaming data (e.g., filtering, aggregation). c. Output the processed data to a sink (e.g., write to a file, a database, or display it).\n",
    "\n",
    "\n",
    "To set up a Spark Streaming application that processes real-time data from a simulated data source or Apache Kafka, we'll walk through the key steps required to ingest data in micro-batches, apply transformations (e.g., filtering and aggregation), and output the processed data to a sink (e.g., writing to a file).\n",
    "\n",
    "Below is a detailed PySpark example using a simulated data source (for simplicity) that simulates streaming data. In a real-world scenario, you could replace the simulated source with Apache Kafka or other real-time sources.\n",
    "\n",
    "Example: Real-Time Data Processing with Spark Streaming\n",
    "We will:\n",
    "\n",
    "Set up a streaming source (simulated).\n",
    "Apply a transformation to filter and aggregate the data.\n",
    "Output the processed data to a sink (in this case, writing to a file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingContext\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Streaming Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Create a StreamingContext with a batch interval of 10 seconds\n",
    "ssc = StreamingContext(spark.sparkContext, 10)\n",
    "\n",
    "# Step 3: Define a schema for the incoming data (simulating a stream of data)\n",
    "schema = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),\n",
    "    StructField(\"Product\", StringType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Step 4: Simulate a streaming source by creating a DStream from a text file source (for demo purposes)\n",
    "# In a real-world scenario, replace this with Kafka or socket stream\n",
    "data_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Step 5: Convert DStream to DataFrame using the schema\n",
    "def process_rdd(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        # Convert RDD to DataFrame\n",
    "        df = spark.read.json(rdd, schema)\n",
    "        \n",
    "        # Step 6: Apply a transformation (filter data where price > 100 and Quantity > 1)\n",
    "        filtered_df = df.filter((col(\"Price\") > 100) & (col(\"Quantity\") > 1))\n",
    "        \n",
    "        # Step 7: Perform aggregation (sum of Quantity per Product)\n",
    "        aggregated_df = filtered_df.groupBy(\"Product\").sum(\"Quantity\").withColumnRenamed(\"sum(Quantity)\", \"Total_Quantity\")\n",
    "        \n",
    "        # Step 8: Output to a sink (write to file in this case)\n",
    "        aggregated_df.write.mode(\"append\").csv(\"output/processed_data.csv\")\n",
    "\n",
    "# Step 9: Apply the processing function to each RDD in the DStream\n",
    "data_stream.foreachRDD(process_rdd)\n",
    "\n",
    "# Step 10: Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Step 11: Await termination of the streaming process\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Explain the fundamental concepts of Apache Kafka. What is it, and what problems does it aim to solve in the context of big data and real-time data processing?\n",
    "\n",
    "Apache Kafka is an open-source distributed event streaming platform designed to handle high-throughput, low-latency real-time data feeds. It is widely used for building real-time data pipelines and streaming applications. Kafka is primarily used to handle and process large volumes of event data, often for real-time analytics, monitoring, and event-driven architectures.\n",
    "\n",
    "Key Concepts of Apache Kafka\n",
    "Producer:\n",
    "\n",
    "Producers are entities that send data to Kafka topics. They are responsible for producing and publishing messages (events) to Kafka brokers. Producers can push data to Kafka at high speeds and are typically integrated with data sources, such as application logs, IoT devices, or microservices.\n",
    "Consumer:\n",
    "\n",
    "Consumers are the entities that read or subscribe to data from Kafka topics. They process and consume the events produced by the producers. Consumers can work independently or in groups (consumer groups) to parallelize data processing. Kafka guarantees that each consumer group will get a distinct subset of data.\n",
    "Broker:\n",
    "\n",
    "Kafka brokers are the servers that store and manage Kafka topics. They handle the writing and reading of events to and from Kafka. Kafka can have multiple brokers, and data is distributed across brokers for fault tolerance and scalability.\n",
    "Topic:\n",
    "\n",
    "A topic is a logical channel to which producers send events and consumers subscribe to. Kafka topics allow data to be categorized and organized. Topics are partitioned, meaning each topic can have multiple partitions distributed across brokers, which enables parallel processing and fault tolerance.\n",
    "Partition:\n",
    "\n",
    "A partition is a single unit of storage for a topic. Kafka topics are divided into partitions, and each partition can be replicated for fault tolerance. Partitions allow Kafka to scale horizontally as the data grows, enabling parallel read and write operations.\n",
    "Consumer Group:\n",
    "\n",
    "A consumer group is a group of consumers that jointly consume messages from Kafka topics. Kafka ensures that each message within a topic partition is consumed by only one member of a consumer group, enabling parallel processing of data while ensuring message delivery guarantees.\n",
    "Zookeeper (Deprecating in Future):\n",
    "\n",
    "Zookeeper is used by Kafka to manage and coordinate its distributed architecture. It helps with tasks like leader election for partitions and broker metadata storage. However, Kafka is moving towards eliminating the dependency on Zookeeper in future versions by introducing a KRaft mode (Kafka Raft Protocol).\n",
    "Kafka's Role in Big Data and Real-Time Data Processing\n",
    "Kafka is designed to address several challenges faced in big data and real-time data processing:\n",
    "\n",
    "High Throughput and Scalability:\n",
    "\n",
    "Kafka is optimized for handling large volumes of high-throughput data streams. It is designed to scale horizontally by adding more brokers and partitions, making it suitable for big data applications where massive volumes of data need to be ingested, processed, and transferred in real time.\n",
    "Fault Tolerance and Reliability:\n",
    "\n",
    "Kafka provides fault tolerance by replicating data across multiple brokers. Each partition has one leader replica and multiple follower replicas, ensuring that even if a broker goes down, the data is still available and can be accessed from other replicas.\n",
    "Real-Time Data Streaming:\n",
    "\n",
    "Kafka is built for real-time data streaming, enabling the ingestion, processing, and consumption of events in real-time. This capability makes it suitable for applications that require low-latency data processing, such as monitoring systems, financial systems, and recommendation engines.\n",
    "Decoupling of Producers and Consumers:\n",
    "\n",
    "Kafka decouples producers and consumers, allowing them to operate independently. Producers can publish events to Kafka without worrying about how or when the consumers will process the data. Similarly, consumers can read data at their own pace without affecting the producers, enabling more flexible, scalable architectures.\n",
    "Durability:\n",
    "\n",
    "Kafka provides durability by persisting all published messages to disk. Data in Kafka is retained for a configurable retention period, meaning consumers can access historical data even after it has been produced. This makes Kafka suitable for use cases such as log aggregation, audit logs, and event replay.\n",
    "Stream Processing:\n",
    "\n",
    "Kafka provides built-in stream processing capabilities with Kafka Streams and ksqlDB. Kafka Streams is a lightweight library for building stream processing applications, while ksqlDB allows users to query and process streams of data in a SQL-like manner. This makes Kafka an ideal solution for building real-time analytics and event-driven applications.\n",
    "Problems Kafka Solves in Big Data and Real-Time Processing\n",
    "Handling High-Volume, Real-Time Data:\n",
    "\n",
    "Traditional data systems are often unable to handle the high throughput and low-latency demands of real-time data processing. Kafka is designed to process millions of events per second, making it suitable for large-scale real-time data streaming applications.\n",
    "Data Integration Across Systems:\n",
    "\n",
    "In a modern data ecosystem, data is often generated by different systems and needs to be integrated in real-time. Kafka allows easy integration across heterogeneous data sources and destinations, such as databases, analytics platforms, and machine learning pipelines, without the need for point-to-point connections.\n",
    "Data Loss Prevention:\n",
    "\n",
    "Kafka's replication mechanism ensures that even in the event of broker failures, data is not lost. Kafka’s durability guarantees ensure that critical data streams can be reliably consumed by downstream systems.\n",
    "Event Sourcing and Auditability:\n",
    "\n",
    "Kafka provides the ability to store every event (message) that is published to a topic, making it an ideal solution for event sourcing architectures and ensuring the auditability of every action taken within a system. This is particularly useful for applications in finance, healthcare, and compliance-heavy industries.\n",
    "Decoupling Producers and Consumers:\n",
    "\n",
    "Kafka allows producers and consumers to operate independently, which helps in building loosely coupled systems. This decoupling enables easier scalability, fault tolerance, and flexibility in handling diverse types of data sources and consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Describe the architecture of Kafka, including its key components such as Producers, Topics, Brokers, Consumers, and ZooKeeper. How do these components work together in a Kafka cluster to achieve data streaming?\n",
    "\n",
    "Apache Kafka’s architecture is based on a distributed, partitioned, and replicated messaging system. It is designed to handle high-throughput, low-latency data streaming and event processing. Kafka is made up of several key components that work together to enable seamless data streaming and messaging.\n",
    "\n",
    "Key Components of Kafka Architecture\n",
    "Producers:\n",
    "\n",
    "Role: Producers are the data sources that publish data to Kafka topics. They send records (events or messages) to Kafka brokers.\n",
    "Operation: Producers can push records to specific Kafka topics, and these records are stored in partitions within those topics. Producers determine which partition a record should go to, often based on a key or a round-robin approach.\n",
    "Load Balancing: Kafka producers can balance load across multiple brokers by choosing different partitions within a topic.\n",
    "Topics:\n",
    "\n",
    "Role: A Kafka topic is a logical channel to which producers send messages and consumers subscribe to read the messages. It acts as a message queue where records are grouped by category.\n",
    "Partitions: Kafka topics are divided into partitions, which allows Kafka to scale horizontally. Each partition is an ordered, immutable sequence of records, and the records within a partition are strictly ordered. Each partition is distributed across Kafka brokers for scalability and fault tolerance.\n",
    "Message Retention: Kafka retains messages for a configured amount of time or until the topic reaches a set size. This retention ensures that consumers can read messages at their own pace.\n",
    "Brokers:\n",
    "\n",
    "Role: Kafka brokers are the servers responsible for storing and managing the messages in Kafka. A Kafka cluster can have multiple brokers, which handle data distribution and storage.\n",
    "Partition Distribution: Kafka brokers store partitions of topics. Each partition is managed by a single broker, but multiple brokers can host replicas of the same partition for fault tolerance.\n",
    "Replication: Kafka ensures fault tolerance by replicating partitions across multiple brokers. Each partition has a leader replica (the primary broker for the partition) and multiple follower replicas (which replicate data from the leader).\n",
    "Consumers:\n",
    "\n",
    "Role: Consumers are the entities that read or subscribe to data from Kafka topics. They process messages sent by producers.\n",
    "Consumer Groups: Kafka allows consumers to join a consumer group, where each consumer in the group reads data from a subset of the partitions. Kafka ensures that each partition is read by only one consumer in a consumer group, allowing for parallel processing of data. If the consumer group grows, Kafka automatically rebalances the partitions across the available consumers.\n",
    "ZooKeeper:\n",
    "\n",
    "Role: ZooKeeper is a distributed coordination service used by Kafka to manage metadata, leader election, and cluster coordination.\n",
    "Metadata Management: ZooKeeper keeps track of Kafka brokers and the partition-to-broker assignments. It helps in managing Kafka’s cluster state and keeps the Kafka brokers in sync.\n",
    "Leader Election: ZooKeeper is used for leader election, ensuring that only one broker is responsible for each partition at any given time (the leader). If the leader broker fails, ZooKeeper helps elect a new leader from the follower replicas.\n",
    "How These Components Work Together in a Kafka Cluster\n",
    "In a Kafka cluster, the producers, topics, brokers, consumers, and ZooKeeper all interact to achieve data streaming. Here is how they work together:\n",
    "\n",
    "Producers Send Data to Topics:\n",
    "\n",
    "Producers send records (messages) to Kafka topics. Each message is stored in a partition within the topic. The producer determines which partition to send a message to, either based on a key (which results in consistent partitioning) or by round-robin distribution for load balancing.\n",
    "Brokers Store Messages in Partitions:\n",
    "\n",
    "Kafka brokers receive the messages from producers and store them in partitions. Each partition is an ordered, immutable sequence of records. Each broker manages one or more partitions. The partitions are replicated across multiple brokers to ensure fault tolerance and availability.\n",
    "ZooKeeper Coordinates Kafka’s Cluster:\n",
    "\n",
    "ZooKeeper helps manage and coordinate Kafka’s cluster. It stores metadata, such as the mapping of partitions to brokers, and ensures that the Kafka brokers are aware of each other’s state. It also handles leader election for each partition (to determine which broker will be the leader for a partition) and helps in reassigning partitions in case of broker failures.\n",
    "Consumers Read Messages from Topics:\n",
    "\n",
    "Consumers subscribe to Kafka topics to read messages. Kafka ensures that messages from each partition are consumed by only one consumer within a consumer group, allowing for parallel processing of messages.\n",
    "Consumers can process messages in order from the partitions, and Kafka guarantees message delivery based on the configured retention policies.\n",
    "Fault Tolerance and Data Replication:\n",
    "\n",
    "Kafka ensures high availability and fault tolerance by replicating data across multiple brokers. Each partition has a leader and several follower replicas. The leader handles read and write requests, while the followers replicate the leader’s data. If the leader broker fails, ZooKeeper ensures that a new leader is elected from the follower replicas to maintain the availability of the data.\n",
    "Rebalancing in Consumer Groups:\n",
    "\n",
    "When a new consumer joins a consumer group or an existing consumer leaves, Kafka rebalances the partition assignments among the consumers. This ensures that each consumer in the group gets a subset of the partitions to consume from, enabling distributed data processing.\n",
    "Data Streaming in Kafka Cluster\n",
    "Kafka achieves real-time data streaming by efficiently ingesting, storing, and distributing data across a distributed system. The producers continuously send messages to Kafka topics, where the messages are partitioned and replicated. Consumers subscribe to these topics to read the messages, process them, and take appropriate actions. Kafka’s distributed architecture allows it to handle large volumes of data in real-time while ensuring fault tolerance and data consistency.\n",
    "\n",
    "ZooKeeper plays a crucial role in managing the Kafka cluster, ensuring coordination and leader election for partitions. By decoupling producers and consumers, Kafka allows for flexible and scalable data pipelines, making it suitable for use cases such as event sourcing, log aggregation, and real-time analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Create a step-by-step guide on how to produce data to a Kafka topic using a programming language of your choice and then consume that data from the topic. Explain the role of Kafka producers and consumers in this process.\n",
    "\n",
    "Step-by-Step Guide to Produce and Consume Data from a Kafka Topic\n",
    "In this guide, we'll use Python with the confluent-kafka library to produce and consume messages from a Kafka topic. This guide will cover how Kafka producers send data to Kafka topics, and how Kafka consumers read data from those topics.\n",
    "\n",
    "Prerequisites\n",
    "Kafka Cluster Setup: You need to have a running Kafka cluster. You can either set it up locally or use a managed Kafka service like Confluent Cloud.\n",
    "Install Python: Ensure Python is installed on your system.\n",
    "Install Kafka Python Client: Install the confluent-kafka library to interact with Kafka from Python.\n",
    "\n",
    "pip install confluent-kafka\n",
    "\n",
    "1. Create Kafka Producer\n",
    "The Kafka Producer sends data (messages) to a Kafka topic.\n",
    "\n",
    "Step-by-Step Instructions:\n",
    "Import the necessary libraries:\n",
    "Use the Producer class from the confluent_kafka module.\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "import socket\n",
    "\n",
    "Define the Kafka configuration for the Producer:\n",
    "The configuration includes properties like the Kafka broker address.\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Kafka broker address\n",
    "    'client.id': socket.gethostname()  # Set the client ID (optional)\n",
    "}\n",
    "\n",
    "Create a Producer instance:\n",
    "The producer is initialized using the configuration defined earlier.\n",
    "\n",
    "producer = Producer(conf)\n",
    "\n",
    "Define a callback function for delivery reports (optional):\n",
    "Kafka producers send data asynchronously. This function provides feedback on whether a message was successfully delivered.\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print('Message delivery failed: {}'.format(err))\n",
    "    else:\n",
    "        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\n",
    "\n",
    "\n",
    "Produce data to Kafka:\n",
    "Use the produce() method to send data to a specific Kafka topic. Here we send a message to the \"test-topic\".\n",
    "\n",
    "producer.produce('test-topic', key='key1', value='Hello, Kafka!', callback=delivery_report)\n",
    "\n",
    "Flush the Producer:\n",
    "Call flush() to ensure that all messages are delivered before the program ends.\n",
    "\n",
    "producer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'confluent_kafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Producer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msocket\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Kafka configuration\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'confluent_kafka'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "import socket\n",
    "\n",
    "# Kafka configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Kafka broker address\n",
    "    'client.id': socket.gethostname()\n",
    "}\n",
    "\n",
    "# Create a producer instance\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Define the delivery callback function\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print('Message delivery failed: {}'.format(err))\n",
    "    else:\n",
    "        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\n",
    "\n",
    "# Produce a message to the 'test-topic'\n",
    "producer.produce('test-topic', key='key1', value='Hello, Kafka!', callback=delivery_report)\n",
    "\n",
    "# Flush the producer\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create Kafka Consumer\n",
    "The Kafka Consumer reads messages from a Kafka topic. It subscribes to topics and processes the incoming messages.\n",
    "\n",
    "Step-by-Step Instructions:\n",
    "Import the necessary libraries:\n",
    "Use the Consumer class from the confluent_kafka module.\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "\n",
    "Define the Kafka configuration for the Consumer:\n",
    "The configuration includes properties like Kafka broker address, group ID, and how to handle offsets.\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Kafka broker address\n",
    "    'group.id': 'my-consumer-group',        # Consumer group ID\n",
    "    'auto.offset.reset': 'earliest'         # Start reading from the earliest message\n",
    "}\n",
    "\n",
    "\n",
    "Create a Consumer instance:\n",
    "The consumer is initialized using the configuration defined earlier.\n",
    "\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "Subscribe to the Kafka topic:\n",
    "The consumer subscribes to the topic(s) from which it wants to consume messages.\n",
    "\n",
    "consumer.subscribe(['test-topic'])\n",
    "\n",
    "\n",
    "Consume messages:\n",
    "The consumer continuously polls the Kafka topic for messages. Here, we'll consume and print the messages in an infinite loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'consumer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     msg \u001b[38;5;241m=\u001b[39m consumer\u001b[38;5;241m.\u001b[39mpoll(\u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Poll for new messages (timeout of 1 second)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'consumer' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsuming stopped manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     consumer\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'consumer' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)  # Poll for new messages (timeout of 1 second)\n",
    "        \n",
    "        if msg is None:\n",
    "            print(\"No message received\")\n",
    "        elif msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print('End of partition reached: {}'.format(msg))\n",
    "            else:\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            print('Consumed message: {}'.format(msg.value().decode('utf-8')))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consuming stopped manually.\")\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
